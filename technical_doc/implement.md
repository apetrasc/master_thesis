# 開発環境と実装・保全

データ解析を行う上で実装したコードは適切に管理する必要がある。知っておくべき技術は以下の3つである。

- Git
- Python
- Pytorch

## コード管理

特に我々チームの研究においてはなぜかプログラミングが当然できるものとして扱われがちなので、最初本当に戸惑うことが多いと思う。どうすればプログラムを実行できるのか、なぜか動かないのだがどうすればいいのかと困る後輩に対し先輩が時間をとって教えるというのも重要なプロセスの一つではあるのだが、無駄が多い。

そこで、できる限りその作業を書面に残しマニュアル化することによって、本質的な指導に専念できるようになるという体制を構築することを目的とした。その観点から最もコード管理に適したサービスとしてGitHubを採用した。ソフト開発の現場では広く用いられるこのサービスだが、機械学習を導入したいという段階の分野においてはまだまだ知名度が低い。具体的なコマンドに関しては、私のページのReadme.mdに記載しているのでそちらを参照されたい。何も考えずコマンドを打ちこむだけで再現が可能になるという体制を構築した。深い知識を必要とせず動かせるということを第一に考えた。また理解できる内容が増えるほど自由に改造できるようになるという利点がある。

## パラメータ管理

機械学習で最も肝要なのは再現性である。説明してもよくわからないとか言われることも多い上、基本統計ベースの予測なので究極的な理由付けは困難であることが多い。やはり機械学習を用いる一番の理由はその精度にあるので、たまたま評価の際によいグラフや数値が出たとしても、それをなくしてしまったらどうしようもない。あるデータセットで学習して、良い結果が出たとしても、その時のコードを保存するのを忘れてしまったり、モデルを上書きしてしまったり、ハイパーパラメータの設定やアーキテクチャを忘れてしまって、再現できなくなることが非常に多い。モデルに関してはGitで管理するならば問題ないと思うが、Epoch数やハイパーパラメータの管理、そして最も肝要なモデルの保存はしっかりやっておかないとすぐに紛失してしまう。

そのような問題に対処し、うまく管理をしてくれるツールとして、hydraというものが挙げられる。Facebook社によって開発されたこのパッケージは、訓練スクリプトが実行された際にその日付と時刻の名前を持つディレクトリを作成し、変数としてそれを格納できる。例えば、2025年10月10日17時10分30秒に訓練スクリプト`train.py`を実行した場合、特定のディレクトリの配下に"2025-10-10/17-10-30"という名前のフォルダを作成し、そこに、ある特定のルールをもってして学習履歴とその際の設定ファイル、saliency mapなどのログや、機械学習のモデルの重みを保存する。これをロードすることにより、いつでもモデルをロードして、予測につかうことができるようになるのだ。これも、重要なのでぜひgithubのページを確認してほしい。

他にも、デプロイまでを一連の操作で自動化してくれるmlflow,ハイパーパラメータを探索してくれるhydraなどいろいろなものがあるが、実装には至っていない。また可能ならば、データも保存しておくのが望ましいだろう。

## データ管理

機械学習では、データのバージョン管理も同様に重要である。データセットの中に不適切なものがあった場合には除外する必要があるし、そういった処理がなされていないものを使用してしまうこともある。その意味で、何かの機械学習モデルを作成しようと思った場合には、やはりオンラインストレージを利用した管理を行うことが望ましい。オンラインでデータセットを監視・管理することによってデータベースを構築し、そのディレクトリ構成や要約をドキュメントとして権威化・ルール化する。そのうえで、そのルールを共有し、データローダを構築したり、ノイズ除去などの前処理をするパイプラインを構築すると良いだろう。そこでデータセットのバージョン管理がなされれれば、実行ログと合わせて完全な再現と調査、改善が可能になる。重要なのは、このあたりの手続きを明文化し書類に起こすことである。「推論」に至っては完璧に再現できるので、機械学習済モデルが提案されたが、良いモデルを得るのは一般に難しい。誤った方向に学習が進んでしまうこともあるが、その理由は学習を開始した際の高次元での誤差関数の初期値付近の表面が不安定だからということが考えられる。そのような場合での証拠を残すために、学習履歴はデータのバージョンまできっちり残しておくのが重要である。しかし、そのような体制にはセキュリティの問題が大きく絡んでくるので実現が難しいことも十分考えられる。この環境に関しては、研究室の方針に大きく左右されると思われるので、データの価値を鑑みながら進めることが望ましい。

そのような体制が効果的である場合として、以下のような事例が考えられる。本研究は3台のワークステーションで行われた。rtx4090が一台積んであるもの、gtx1080が3台積んであるもの、A4000が2台ついてあるものとなる。これらが同時に高い稼働率を持っていることが望ましいが、例えばどのデータセットをロードするのかが問題となる。今回は暫定的にそれぞれに安価な外付けハードディスクドライブを取り付けて、そこにデータをダウンロードして変換、訓練及び評価に利用したが、データに何らかの問題が生じた場合に、これら3台のPCに手作業で同じ操作を施す必要があった。オンライン上に最新のものが構築されていた場合には、そこからバージョン履歴とともにデータセットをダウンロードしておくスクリプトをgitで管理しておけば、コマンドひとつで同期できるだろう。

余談だが、ローカルドライブを同期できるサービスは、筆者の知る限り存在しない。同期させるのであればオンラインにそのコピーを保存する必要があるだろうし、意味がないからだと考えられる。課金してGoogleDriveやAWSなどを用いたほうがいいだろうし、実際の多くの企業はそうしている。個人用のドライブでは、上限が15GB程度でとてもではないが不足する。NASも使用していたが、古かったためか頻繁にクラッシュする事態に遭遇した。また、Droboは公式でWindowsにのみ対応しており、Linuxでドライブを認識させるには自前でパッチを用意する必要があり断念した。余力のある読者諸賢に委ねたい。

## 開発体制

私よりも一つ上の代での共同研究の体制では、実機データをそのまま機械学習に通すというアプローチが採用されていた。そして、評価指標・目標についての共有は不十分であった。さらに、データがあまりに少ないために、Grad-camの画像をもとに有益な解釈や結論を得るのが難しいという欠点もあった。それらに対し、一般に機械学習の世界での経験則として「前処理に注力することで精度が向上する」というものがあるため、ヒルベルト変換・バンドパスフィルタの適用という思考に至ったと考えられる。

これらに対しては、無論データが有限である状況下において最大限の結果を残すという意味で有益ではあるものの、そのデータを増やすことはできないのか、何か別の議論が発生したとして、それらに対処する案をより多角的に検討することが重要である。その観点から、他のグループで使用されている数値計算ソフトウェアを応用してデータを取得するという発想に偶然至り、本研究がなされた。

LLMの学習データが不足しているのか、はたまたプロンプトが悪いのかは不明だがGPTGPTモデルが出力する数値計算のコードは間違っていたり、うまく動かないことが極めて多かった。しかし、それでも利用する意義は大きい。以下にその利点を示す。

- 実機データの改善要素のうち、改善不可能なものと対応可能であるものが一部明確になる。
- シミュレーションデータも、一部改良を行うことによって機械学習データセットとして利用できる。

Grad-camの画像を確認する際、それが計測機器依存のバリアンスなのか、物理現象として重要な原因なのかを判断することができないという事態が多くみられた。議論の材料さえ不足しており、機械学習のデータセットが不足している状況下においては十分考えられることである。

そこで、数値計算ソフトウェアに基づいて計測信号を取得することによって、いくつかの有益な結論を得ることができる。実験系についての論文に基づいて数値計算の前提条件を詳細に設計することにより、支配方程式に基づく理想的な場合の計測データを得ることが可能となるのだ。これにより、シミュレーションに見られる定性的・定量的な特徴と実機測定データにおける同じ特徴量を比較することが可能となり、現象論として原因の究明に寄与することができるのである。さらに、鉛直管内上昇固液二層といった限定的な場合においてはシミュレーションによって生成した信号波形を直接学習し、その予測モデルを利用して比較的良い精度を達成することができた。

無論、数値計算結果の章にあるような解析・比較も行い、いくつかの特徴量が統計的に等しいデータセットを学習させてしまっては、事実上実機データを学習させているのと変わらないのではないかという指摘・懸念もありうる。しかし、それでも機器依存のノイズといった再現不可能な量も確かに存在し、それらに対してロバストな学習を行うことができたという意味で、シミュレーションデータセットを学習させる意義はあるだろう。

総じて、今後はシミュレーションによってデータを生成し、コードと生成結果を厳密にチェックし、それらをパスした場合のみ学習させるという開発体制が考えられる。そこで、テストは厳密かつ詳細に設計されなければならない。今回も、結果として大きな問題ではないことが示されたものの、いくつかのエラーが示された。今後小さなヒューマンエラーが結果を大きく覆すような事態を引き起こす可能性も十分ある。そこで、実施が望まれるテストを各フェーズに応じて提案する。その内容を以下に示す。

### 数値計算

前述の通り、今回は座標を適切な確率分布からのサンプリングにより生成し、そこに固相を配置し別々な配置パターンでシミュレーションを行うという手続きをとっていると述べた。ここで、今回は残念ながら固相粒子がわずかに重なってしまい、それを発見できなかった。

開発体制としては、まず固相が存在しない単相流のシミュレーションを行い、それらを比較するというアプローチをとった。ここで、信号波形に現れるピークは、超音波パルスの管壁外側の反射、内側の反射、対向側の管壁内側の反射、外側の反射であることは物性値および信号の時間から確認できる。このピークとピークの間の距離を測ることによって、計算系の妥当性を検証してある。

次に、ガラス球の半径を実験資料から確認できる値に設定し、1つ、2つ程度真ん中に置いたり、中心から少しずらしたような場所に配置した。これらにより、中心付近にその兆候がみられることを確認した。さらに、ガラス球の座標を変数化し、指定された位置にガラス球が配置されることも確認した。これらのテストにより、ガラス球の座標のセットを入力として数値計算を行う関数が作成されたことが確認されたとみなした。

一方で、その座標に関してのテストも行った。前述の通り、$\{x | x\in[-1,1]\},\{y | y\in [-1,1]\},\{z |z \in [0,1]\}$の区間で生成されているが、これらのテストとして$\min_{i,j\in m}d_{ij}, d_{ij}=||r_i-r_j||_2$の値を計算した。これらが適切な閾値よりも上だった場合には、これらは固相として計算系で互いに重ならないことが期待される。シミュレーションに使用した元の値はこれらのテストをパスしていたが、これらを実際の計算系に適用する過程、つまり、[mm]のスケールに合わせて拡大する際の試験を怠っていた。それゆえ、ガラス球がわずかとはいえ重なりあう結果を許してしまった。次回以降、より大規模な開発の際はその試験も追加すべきである。

### 信号処理

基本的にここで扱う処理はフーリエ変換、ヒルベルト変換、信号整形のみである。フーリエ変換のような長い歴史を持つ関数は十分テストされつくしてきたであろうのでテストは不要とした。問題は信号整形である。高次テンソルを扱う都合上、そこからの行列の抽出の際にミスが発生しうるので十分テストされなければならない。

特に、実機データにおける処理が肝要である。シミュレーションは実機に合わせればよいが、実機のデータを変換する際は過不足なく情報を抽出せねばならない。これらのファイルに含まれるデータのうち、メタデータを除けばチャンネル数を$C$、信号点数を$L$として$\mathbb{R}^{1\times L \times C}$である。 これらを、測定時に応じて$H$が総測定時間での測定回数と一致するように$\mathbb{R}^{H\times W \times C}$に変換し、測定開始時から無意味と考えられるデータを除外した。この際、チャンネルごとに操作を実施するわけだが、1次元の信号系列から測定時刻を検出し、その時刻を起点として$\mathbb{R}^{H\times W}$に変換したのである。ここで、時刻が正しく検出されているかに関しては、Hのサイズを表示することで確認した。これらが正しく$15000$と表示されていたため実装が正しいとみなした。しかし、解説の通りきわめて自由度の低い柔軟性のない処理であり、よりよい実装が期待される。

ヒルベルト変換に関しては、ヒルベルト変換後の信号波形をnp.ndarray配列に格納し、赤でプロットすることによって正しさを検証した。ヒルベルト変換は信号包絡線を取得する目的で行っていたため、実際の信号と重ねてプロットすることによって処理の正しさを視覚的に確認し、正しいとみなした。処理の内容自体に誤りはないと考えられるので、その利用例を注意深く観察し、問題がないと確認した。

### 機械学習

2025年現在、LLMに書かせたコードがエラーなく動くことも多く、簡単なプロンプトを渡すことによってよい結果を出力してくれることは多い。事実、LLMの開発に機械学習を多用しているせいかLLMが出力する機械学習のコードの完成度はほかの分野に比べて高いような感触さえある。しかし、以前として人間がそのコードについて責任を持たねばならず、それを検証する作業が必要である。理論について詳細な理解もなしに開発を進めてしまうと、先入観や思い込みに結果が引きずられてしまう恐れもある。たとえば、「エラーが起きないこと」を目標に開発を行い、コードを理解できる知識なしにGPT駆動の開発を行っていると、テスト用スクリプトの実施を消去する挙動に気づけない。この現象はいくつか報告されており、これを避けるために日々研鑽する必要があるだろう。このあたりを区別するのは一般に難しい。

# ソフトウェアの仕様、選定について

ある機能を実現する上で、いくつかのアプローチが考えられるものである。ソフトウェアにおいても例外ではない。ソフトウェアの性質として、計算機上での表現である以上再現性が担保されているというものがあり、教育目的ですべてを自力で実装しなおすという姿勢も十分考えられるだろう。しかし、これらはあまりにも非効率であることが多い。開発には暗黙知が多く、自分で開発したツールは既存のものの劣化版になる可能性が非常に高い。他者が開発したツールは積極的に活用して、できるだけ少ない労力で効率的に目的を達成すべきなのである。その意味で、ライブラリは積極的に活用し、自分が書くコードは最小限にとどめておくほうが無難である。しかし一方で、世には同じ機能をもつが違う名前を持つツールが多数存在する。最終的に実現する内容が同じであれど、ライブラリ毎に内部構造の洗練度に違いがあったり、高速化モジュールが組み込まれていて速度に大きな変化をもたらしたりすることは多い。本付録においては、本論文の目的を達成する上で2025年時点において人気なツールとその性質を簡単に比較し、選定の理由を示す。

## 数値計算

使用したコードを[GitHub](https://github.com/apetrasc/kwavesource.git)に示す。

引用の通り、kwaveを使用している。実装にはpython,matlabの環境が提供されているが、再現性や研究室のユースケースを考えmatlabを選択した。2025年時点ではAirliftプロジェクト以外にも研究室の多くのメンバーがkwaveを利用して超音波信号波形のシミュレーションを行っていたが、Pythonの環境を利用している者はいなかった。ソフトウェア開発においてはエラーの解決に手間取ることが大変に多いので、近くに似たような事例に遭遇した人がいると大きな助けになることが多い。研究室のメンバーと同じ環境を構築すると利点が大きいので、Matlabベースでの開発を選択した。

## 信号処理

処理の全体を[GitHub](https://github.com/apetrasc/psdata2matlab.git)に示す。

実際の計測信号は、.psdataと呼ばれる形式で保存されている。この.psdata形式はかなり独自性の強い拡張子であり、2025年時点でpicoscopeという公式のソフトウェア以外は件のデータをロードすることさえできない。さらに公式ソフトウェアに対しても機能を統合し一括で処理を行うこともできない。そこで、何らかの形式に変換する必要があった。

ここで、picoscopeにはデータ変換機能が装備されておりその変換後の拡張子にはいくつかの候補があった。従来は.csvに変換し、pandasといったデータ解析ライブラリで読み込み処理するという体制をとっていた。しかし、これでは変換後のファイルサイズが一つあたり7GB程度になるほか、変換に多大な時間を要する問題点があった。確かに、.csvはKaggleといったデータサイエンスコンペティションプラットフォームで広く用いられる拡張子である。しかし、それでもトータルサイズが1GBを超えるのは稀である。今回は一つのファイルが7GBほどのサイズになってしまうので、トータルでは優に1TBを超えてしまう。これでは機械学習のデータを読み込むだけでも多くの時間がかかってしまい、開発に支障をきたす。そこで、バイナリ形式の.matを変換後の拡張子として選択した。これは、名前の通りmatlabで解析もできるほか、pythonの信号処理ライブラリであるscipyにも互換性があるのでデータの変換が容易であるというメリットもある。

信号処理を行う際のプログラミング言語、ライブラリの選定も重要である。ここでは、デプロイのしやすさと筆者が慣れているということもあってpythonを選択した。幸い、pythonでは数多くの信号処理ライブラリが存在する。また、近年の機械学習ライブラリは一般にPythonで書かれているので、スクリプトのリファクタリングや処理関数を共有できることができるという点も大きな強みである。信号処理のライブラリとしては、代表的なものにlibrosa, scipy.signal,torchなどがある。このうち、ヒルベルト変換を行うことが必達であるので、その機能が提供されているscipy.signalも利用している。ただ、この問題点としてGPUを使った高速化ができないというものが挙げられる。実機データに対しては15000×5208の信号系列をヒルベルト変換する必要があり、CPUだと決して無視できない時間がかかってしまう。この問題点を解決するため、やむなくtorch環境でのGPUを利用したヒルベルト変換を実装し、利用している。超音波画像法を利用する上ではヒルベルト変換は不可欠であるため、その実現として以上にあるツールを採用している。

拡張子の問題はさらに存在する。Kwaveでは、シミュレーション結果をPicoscopeの変換先である.matの形式で出力することを可能とする。しかし、メタデータはpicoscopeで変換した場合と大きく異なってしまうために同じ関数や機能で信号波形をプロットすることはできない。無論、実機用とシミュレーション用で別々の可視化スクリプトを設計するというアプローチは考えられる。しかし、これでは物理量の厳密な比較の妨げになるだけでなく表示法がずれることが予想され、本質的な違いに注目する妨げにな。故に両者を統一的な.npz形式に変換した。これらは線形代数ライブラリであるNumpyでロードが可能であり、行列の型と統計量をそろえることで区別なくプロットを行うことができると考えた。命名記法としては、（元データの名前）.npyが存在した場合に、（元データの名前）\_processed.npzとなるようなファイルを生成するスクリプトを実機・シミュレーションそれぞれで設計して呼び出すという体制をとった。他によい体制を思いついた方がいれば、ぜひ改善してほしい。

関数の命名規則、返り値の型まで詳細にルール化することによって互換性をもたせることができる。

## 機械学習

研究に使用したコードを[GitHub](https://github.com/apetrasc/ml_airlift.git)に示す。

近年、機械学習の技術が各分野で頻繁に応用され、様々な技術的課題を解決したことが報告されている。その流れを受けて、各分野でも多くの研究がある。しかし、そもそも機械学習を使う必要があるのかについては詳細に議論されなければならない。数理・統計理論、計算機環境の知識を高度に組み合わせる必要のある技術であるため、「なんでも解決してくれる技術」というような誤った理解のまま開発を進めてしまうと高確率で失敗に終わる。近年、生成AIによって実装は以前よりはるかに簡単になったが、依然として万能ではなく、それに気づかないままできるという前提で進めてしまうと技術員が疲弊する結果になりかねない。以下に機械学習の援用が効果的な場合を示す。

1. そもそも、何らかの手段でその一部が可能であるとわかっているとき
2. データが存在するとき
3. 明確な理論を構築するのが難しい時

データが存在しないのにも関わらず機械学習を使用することはできない。故に計算機科学の世界では、データセット生成だけでも論文になりうる。それでも機械学習を用いようとした場合にはデータセットを新たに開発する必要があり、コストが膨大になってしまう。

研究目的では、可読性と保守性の観点からFacebookが開発した深層学習ライブラリであるpytorchを用いるのが最も望ましいと考え、本研究においてもこのフレームワークを用いて実装を行った。このフレームワークはAPIが豊富であり、基礎的なMLP（多層パーセプトロン）やCNN（畳み込みニューラルネットワーク）というモデルだけでなく、ベイズ統計の基礎を成すガウス過程や、近頃人気の生成モデルを設計するためのVAE（変分オートエンコーダ）などのモデルを容易に実装できるようになっている。Pytorchによって記述されたコードは人間による解釈が容易であり、かつ環境構築が非常に容易であるという観点から採用を行った。他には、Google社によるTensorflow,かつて世界の先駆けだった日本スタートアップPFN社製のChainerなどがある。これらに関して、Tensorflowは筆者の経験上再現が極めて煩雑であったり、同じコードでも動かないということが頻発するので、推奨しない。実体験をもとに語れば、筆者の留学先ではTensorflowを用いた開発を行っていたのだが、全く同じコードのまま2021年という比較的新しい時期に書かれたのにも関わらず、実行できないという事態に遭遇した。これは、開発者本人によるとサーバー上で環境を設定する際に、当時のモジュールが消失してしまっただけではなく、TensorflowのAPIの変数名と機能が変わってしまったがために起きたことであったようだ。GPTモデルをふんだんに駆使して実行してもうまくいかなかったので、開発者に相談して、深層学習を本でしっかり勉強して一行一行書き直して事なきを得た。他方、Pytorchではそういったことがない。極めて数学的に実装する必要がある以上多少書くのが難しいというデメリットはあるが、機能が途中で大幅に変更されて動かなくなるといったことは筆者の知る限り聞かない。

以上の理由により、Pytorchの利用を進める。研究分野においてTensorflowよりもPytorchのほうが4倍多い利用者を持つ。